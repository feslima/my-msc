% !TeX root = ../msc-thesis.tex
\documentclass[../msc-thesis.tex]{subfiles}

\begin{document}

\chapter{The \soc overview}

Every  industrial process is under limitations ranging from design/safety (e.g. 
temperature or pressure which an equipment can operate, etc.), environmental 
(e.g. pollutant emissions), to quality specifications (e.g. product purity), 
and economic viability. More often than not, these constraints are applied all 
at once and can be conflicting. Therefore, it is mandatory to operate such 
processes optimally (or, at least, close to its optimal point) in order to 
attain maximum profits or keep expenses at minimum while still obeying these 
specifications.

One way to achieve this is through the application of plantwide control 
methodologies. In particular, \soc \cite{Morari1980,Skogestad2000,Alstad2009} 
is a practical way to design a control structure of a process following a 
criterion (for instance: economic, environmental, performance) considering a 
constant set-point policy \cite{Alves2018}. The SOC methodology is 
advantageous in this scenario because there is no need to reoptimize the 
process every time that a disturbance occurs.

However, the review presented here contains merely the paramount elements 
needed to understand the main concepts and expressions that translate the 
ideas behind the method. The author recommends them if the reader needs a 
more detailed explanation \cite{Skogestad2000,Halvorsen2003,Hori2005,Hori2007,
Alstad2009,Alves2018,Kariwala2008,Kariwala2009,Umar2012}.

The main concept of Self-optimizing control consists in the pursue of a 
control structure that is based on a constant setpoint policy, leading to 
near-optimal operation. From \textcite{Skogestad2004}:

\begin{citacao}
``Self-optimizing control is when one can achieve an acceptable loss with 
constant setpoint values for the controlled variables without the need to 
reoptimize when disturbances occur.''    
\end{citacao}

It is assumed the process objective function, assumed scalar, is influenced by 
its steady-state operation. Therefore, the optimization problem described in 
\autoref{soc:optproblem} is formed, with $u_{0}$ being the degrees of freedom 
available, $x$ and $d$ representing the states and the disturbances of the 
system, respectively.

\begin{equation}
	\begin{aligned}
	& \underset{u_{0}}{\text{minimize}}
	& & J_{0}\left(x, u_{0}, d\right) \\
	& \text{subject to}
	& & g_{1}\left(x, u_{0}, d\right)=0 \\
	& & & g_{2}\left(x, u_{0}, d\right) \leq 0
	\end{aligned}
    \label{soc:optproblem}
\end{equation}
\nomenclature[B]{$u_{0}$}{Process degrees of freedom}
\nomenclature[B]{$x$}{Process states}
\nomenclature[B]{$d$}{Process disturbances}
\nomenclature[B]{$J$}{Process objective function}
\nomenclature[B]{$J_{0}$}{Process objective function in unrestricted space}

Regarding the disturbances, these can be: change in feed conditions, prices 
of the products and raw materials, specifications (constraints) and/or changes 
in the model. Using NLP solvers, the objective function can be optimized 
considering the expected disturbances and implementation errors.
\nomenclature[A]{NLP}{Non Linear Problem}

Since the whole technology considers near-optimal operation, as a result of 
keeping constant setpoints (differently from RTO, for instance), there will 
always exist a (positive) loss, given by \autoref{eq:generalloss}
\nomenclature[A]{RTO}{Real Time Optimization}

\begin{equation}
	L=J_{0}(d, n)-J_{opt}(d)
	\label{eq:generalloss}
\end{equation}
\nomenclature[B]{$L$}{Loss}
\nomenclature[B]{$n$}{Implementation error}
\nomenclature[B]{$J_{opt}$}{Optimal value of $J$}

\mtc focus on the first four steps of the \soc technology, named by 
\textcite{Skogestad2000} as ``top-down'' analysis. In these steps, the 
variable selection seeking the usage of the steady-state degrees of freedom 
is the main problem to be addressed with the systematic procedure proposed. It 
is possible to search for a \soc structure basically using two methods:

\begin{enumerate}
    \item Manually testing each CV candidate, reoptimizing the process for 
	different disturbances' scenarios, and choosing the structure that yields 
	the lowest (worst-case or average-case) loss; \label{soc:method1}

	\item Using local methods based on second-order Taylor series expansion of 
    the objective function, that are capable of easily and quickly 
    ``pre-screening'' the most promising CV candidates. \label{soc:method2}
\end{enumerate}
\nomenclature[A]{CV}{Controlled Variable}

The manual nature of method \ref{soc:method1} and the possibility of creating 
an automated framework using method \ref{soc:method2} motivated the creation 
of \mtc itself. Applying, comprehensively, the second method in a software was 
also a key motivation for this work. Therefore, it is logical that the usage 
of the linear methods will be discussed in this section, since they are the 
ones implemented within \mtc.

A linear model with respect to the plant measurements can be represented as 
\autoref{eq:linear1}

\begin{equation}
	\Delta y=G^{y} \Delta u+G_{d}^{y} \Delta d
	\label{eq:linear1}
\end{equation}
\nomenclature[B]{$y$}{Measurements}
\nomenclature[B]{$G^{y}$}{Gain matrix with respect to the measurements}
\nomenclature[B]{$G_{d}^{y}$}{Gain matrix with respect to the disturbances}

With

\begin{equation}
	\begin{array}{l}
		{\Delta y=y-y^{*}} \\
		{\Delta u=u-u^{*}} \\
		{\Delta d=d-d^{*}}
		\end{array}
	\label{eq:linear2}
\end{equation}
\nomenclature[B]{$u$}{Manipulated variable}

$G^{y}$ and $G^{y}_{d}$ are the gain matrices with respect to the measurements 
and disturbances, respectively. Regarding the CVs, linearization will give 
\autoref{eq:cvlinear}

\begin{equation}
	\Delta c=H \Delta y=G \Delta u+G_{d} \Delta d
	\label{eq:cvlinear}
\end{equation}
\nomenclature[B]{$H$}{Linear combination matrix}

With
\begin{equation}
\begin{array}{l}
	{G=HG^{y}} \\
	{G_{d}=H G_{d}^{y}}
	\end{array}
	\label{eq:linear3}
\end{equation}

Linearizing the loss function results in \autoref{eq:linearloss}:

\begin{equation}
	\begin{aligned}
		L &=J(u, d)-J_{o p t}(d)=\frac{1}{2}\|z\|_{2}^{2} \\
		z=& J_{u u}^{\frac{1}{2}}\left(u-u_{o p t}\right)=J_{u u}^{\frac{1}{2}} G^{-1}\left(c-c_{o p t}\right)
	\end{aligned}
	\label{eq:linearloss}
\end{equation}
\nomenclature[B]{$z$}{Loss variable}
\nomenclature[B]{$c$}{Controlled variable}
\nomenclature[B]{$J_{uu}$}{Hessian of cost function with respect to the 
manipulated variables $\left(\frac{\partial^{2} J}{\partial^{2} u}\right)$}
\nomenclature[B]{$J_{ud}$}{Hessian of cost function with respect to the 
disturbance variables $\left(\frac{\partial^{2}J}{\partial u\partial d}\right)$}

Later, \textcite{Halvorsen2003} developing the exact local method, showed that 
the loss function can be rewritten as in \autoref{eq:lossexactz}

\begin{equation}
	z=J_{u u}^{\frac{1}{2}}\left[\left(J_{u u}^{-1} J_{u d}-G^{-1} G_{d}\right) \Delta d+G^{-1} n\right]
	\label{eq:lossexactz}
\end{equation}

With $J_{ud}$ and $J_{uu}$ corresponding to the hessian with respect to the 
disturbances and manipulated variables 
$\left(\frac{\partial^{2} J}{\partial u \partial d}\right)$ and with respect to 
the manipulated variables $\left(\frac{\partial^{2} J}{\partial^{2} u}\right)$,
respectively. If one assumes that $W_d$ is a (diagonal) magnitude matrix that 
considers the disturbances and $W_{n}^y$ the magnitude matrix that takes into 
account the measurement error, and considering that both are 2-norm-bounded 
(\textcite{Halvorsen2003} and \textcite{Alstad2009} contains a discussion 
and justification for using 2-norm), \Crefrange{eq:2norm1}{eq:2norm3} can be 
defined to scale the system:

\begin{equation}
	d-d^{*}=W_{d} d^{\prime}
	\label{eq:2norm1}
\end{equation}
\nomenclature[B]{$W_{d}$}{Diagonal magnitude matrix of disturbances}
\nomenclature[B]{$W_{n}$}{Diagonal magnitude matrix of measurement errors}

\begin{equation}
	n=H W_{n}^{y} n^{y^{\prime}}=W_{n} n^{y^{\prime}}
	\label{eq:2norm2}
\end{equation}
\nomenclature[B]{$n^{y^{\prime}}$}{Implementation error with respect to the 
measurements}

\begin{equation}
	\left\|\left(\begin{array}{l}
		{d^{\prime}} \\
		{n^{y^{\prime}}}
		\end{array}\right)\right\|_{2} \leq 1
	\label{eq:2norm3}
\end{equation}

The loss function from \autoref{eq:linearloss} can be also written in a more 
appropriate way considering the definition of \cite{Alstad2009} of the 
uncertainty variables regarding the contribution of the disturbances and 
measurement error on the incurred loss, \autoref{eq:linearM} and considering 
the scaled system from \Crefrange{eq:2norm1}{eq:2norm3}

\begin{equation}
	M \triangleq\left[M_{d} \quad M_{n}^{y}\right]
	\label{eq:linearM}
\end{equation}

where

\begin{equation}
	\begin{aligned}
		&M_{d}=-J_{u u}^{1 / 2}\left(H G^{y}\right)^{-1} H F W_{d}\\
		&M_{n^{y}}=-J_{u u}^{1 / 2}\left(H G^{y}\right)^{-1} H W_{n^{v}}
		\end{aligned}
	\label{eq:linearms}
\end{equation}
\nomenclature[B]{$F$}{Optimal measurement sensitivity matrix with respect to 
the disturbances}	

with $F$ corresponding to the optimal measurement sensitivity matrix with 
respect to the disturbances.

Finally, if one uses all the definitions described so far, the worst-case loss 
for the effect of the disturbances and measurement error is given by 
\autoref{eq:lossM}

\begin{equation}
	L_{worst-case} = \max _{\left\|\left(\begin{array}{l}
		{d^{\prime}} \\
		{n^{y^{\prime}}}
		\end{array}\right)\right\|_{2} \leq 1}=\frac{\bar{\sigma}(M)^{2}}{2}
	\label{eq:lossM}
\end{equation}

\autoref{eq:lossM} shows that in order to minimize the worst-case loss, it is 
necessary to minimize $\bar{\sigma}(M)$, \autoref{eq:argminH}:

\begin{equation}
	H=\arg \min _{H} \bar{\sigma}(M)
	\label{eq:argminH}
\end{equation}

This optimization problem was initially solved using a numerical search, as 
proposed by \textcite{Halvorsen2003}. Fortunately, \textcite{Alstad2009} 
derived an explicit solution that gives the optimal linear combination of 
measurements coefficient matrix (H) that minimize the worst-case loss that 
exists due to the effect of the disturbances and measurement errors, in 
\autoref{eq:Hexact}

\begin{equation}
	H^{T}=\left(\tilde{F} \tilde{F}^{T}\right)^{-1} G^{y}\left(G^{y T}\left(\tilde{F} \tilde{F}^{T}\right)^{-1} G^{y}\right)^{-1} J_{u u}^{1 / 2}
	\label{eq:Hexact}
\end{equation}

where

\begin{equation}
	\tilde{F}=\left[F W_{d} W_{n}^{y}\right]
	\label{eq:Ftilde}
\end{equation}

Assuming that $\tilde{F} \tilde{F}^{T}$ is full rank.

\autoref{eq:Hexact} has three interesting properties proved by 
\textcite{Alstad2009}: 

\begin{enumerate}
	\item It applies to any number of measurements ($n_{y}$).
	
	\item The solution for H was proved to minimize not only the worst-case, but 
	also the average-case loss. Therefore, if one uses \autoref{eq:Hexact} 
	seeking the determination of a control structure that minimizes the loss at 
	the worst-case scenario, he is also minimizing the loss for the average-case 
	scenario. This was called as a ``super-optimality'' by \textcite{Alstad2009}.

	\item The solution proposed minimizes the \textit{combined} effect of the 
	disturbances and the measurement errors, simultaneously.
\end{enumerate}

Therefore, the usage of the explicit solution will give both the minimized worst 
and average case losses using a single evaluation, and will also consider the 
combined effect of the disturbances and measurement errors of the problem. 
Therefore, this solution it is the default one used in \mtc.

Another way of solving the optimization problem from \autoref{eq:argminH} is to 
use the Extended nullspace method \cite{Alstad2009}. Differently from 
\autoref{eq:Hexact}, this solution does not consider the combined effect of the 
disturbances and measurement errors simultaneously. Instead, the problem is 
solved in two steps. The first regards ``disturbance rejection'': The loss is 
minimized with respect to disturbances. If there are remaining degrees of 
freedom, then the effect of the measurement errors can be minimized. The 
extended nullspace, differently from the exact local method, is not an optimal 
solution, instead being considered sub-optimal. \cite{Alstad2007,Alstad2009}. 
However, the authors of \textcite{Alves2018} also translated the mathematical 
formulations of the extended nullspace method into Python, and it is intended 
to be implemented within \mtc GUI in future releases merely as a secondary 
feature, giving its sub-optimality. The solution using the extended nullspace 
method is depicted in \autoref{eq:nullspace}:

\begin{equation}
	H=M_{n}^{-1} \tilde{J}\left(W_{n^{y}}^{-1} \tilde{G}^{y}\right)^{\dagger} W_{n^{y}}^{-1}
	\label{eq:nullspace}
\end{equation}

Since \autoref{eq:Hexact} also minimizes the worst-case loss, its evaluation 
was also considered inside \mtc: the user can inspect the expected 
average-case loss for each control structure that can exist in the 
combinatorial problem. The expression for the average-case loss is a result of 
the work of \textcite{Kariwala2008} and is described in \autoref{eq:avgloss}:

\begin{equation}
	L_{\text {average}}=\frac{1}{6\left(n_{y}+n_{d}\right)}\left\|J_{u u}^{\frac{1}{2}}\left(H G^{y}\right)^{-1} H \widetilde{F}\right\|_{F}^{2}
	\label{eq:avgloss}
\end{equation}

Lastly, it was necessary to implement within \mtc a branch-and-bound algorithm 
capable of quickly searching the best control structures for each possible 
subset of a given process, using the incurred loss as metric. This was 
considered by the authors of \textcite{Alves2018} as an obligatory feature, 
since when \mtc is being used, it was understood that the main idea was to, 
in a comprehensive software, the user operating it should be capable of 
inspecting the most promising control structures, and discarding the 
unnecessary evaluation of the unpromising structures (i.e.: With a high 
incurred loss - both average of worst-case scenario) to save time and effort. 
It is important to remember that there is an evident combinatorial problem 
that grows in an explosive fashion, as the number of the unconstrained 
degrees of freedom of the reduced space problem and the number of available 
measurements both increases. Without a search method that is capable of 
quickly discarding undesired solutions, the usability of \mtc would be 
seriously compromised. Luckily, there are several implementations 
of branch-and-bound algorithms tailored for \soc studies purposes, such as 
in \textcite{Cao2005,Cao2008,Kariwala2009}.

From the aforementioned works, \textcite{Kariwala2009} it is of particular 
interest: the monotonic criterion implemented consists of the exact local 
method from \textcite{Halvorsen2003} and derived explicitly by 
\textcite{Alstad2009}, which is used as the default methodology to pre-screen 
the most promising self-optimizing CV candidates in \mtc. Therefore, the usage 
of the proposed branch-and-bound algorithm by \textcite{Kariwala2009} it is 
not only convenient, making the software more effective, but also keeps the 
``calculation engine'' from \mtc using the same criterion. It would not make 
any sense, for instance, using a branch-and-bound algorithm that outputs the 
index of the most promising CVs using the maximum singular value rule from 
\textcite{SkogestadBook} and use the CV index sequence from this algorithm 
to evaluate the worst-case loss. Fundamentally speaking, the orders of 
``best'' control structures would not be the same, simply because the search 
method would be using an different criterion from the linear method 
implemented to evaluate the $H$ matrix.

The Branch-and-Bound algorithm developed by \textcite{Kariwala2009} that was 
originally implemented in MATLAB\textsuperscript{\textregistered} by them was 
translated to Python by the main author of \textcite{Alves2018}. The same is true 
for equations of Exact Local and Extended Nullspace methods described by 
\textcite{Alstad2009}. Those Python routines were packaged under the name 
of \textit{pySOC} (Python-based \soc), and can be found in 
\url{https://github.com/feslima/pySOC}, with the code being freely available 
for inspection, revision and suggestions.

\end{document}